## 10.4 红队演练与自动化评估

红队演练（Red Teaming）与自动化评估是验证 LLM 系统安全性的重要手段。前者强调“以攻击者视角发现真实问题”，后者强调“可重复、可量化、可回归”的持续评测能力。

### 10.4.1 红队演练的目标与范围

**常见目标**：
- 发现提示注入、越狱、数据泄露、工具滥用等高风险路径
- 验证纵深防御是否存在单点失效与“策略空洞”
- 评估在真实业务任务下的安全-体验权衡

**范围定义**（建议形成书面 ROE：Rules of Engagement）：
- 测试对象：模型、系统提示、RAG、工具链、权限与审计、业务流程
- 数据边界：哪些数据可用于测试、哪些属于“绝对禁止触碰”的生产机密
- 操作边界：哪些动作需要人工审批/隔离环境执行（例如写库、发外部消息）

### 10.4.2 组织方式：从一次性到常态化

红队不应只做“上线前一次”。推荐节奏：
- **上线前**：覆盖关键业务路径的基线红队
- **版本迭代**：系统提示/工具/检索语料变更后触发回归
- **季度/半年度**：结合新威胁情报做专题演练

同时建议引入蓝队/产品/法务参与，保证问题能转化为可落地的工程改进与合规证据。

### 10.4.3 测试用例库：把经验固化成资产

把“红队发现的问题”沉淀为可回归的测试用例库，最有价值。建议对用例做统一结构化描述：
- 场景：业务任务与上下文（客服、投研、代码助手等）
- 攻击面：输入、外部内容、工具调用、长上下文、跨会话记忆
- 期望行为：拒绝/降权/脱敏/触发人工审核/记录审计事件
- 证据：触发条件、日志字段、策略命中、操作类型（在合规边界内）

### 10.4.4 自动化评估：指标与门禁

自动化评估的价值在于“持续、可量化”。建议将以下指标纳入发布门禁：

| 维度 | 指标示例 | 说明 |
|------|----------|------|
| 安全有效性 | 违规输出率、注入成功率 | 越低越好 |
| 隐私保护 | PII 泄露率、系统提示泄露率 | 需要有可测基准 |
| 工具安全 | 未授权工具调用率 | 与权限系统联动 |
| 体验成本 | 误拒率、平均延迟变化 | 避免“安全把业务打死” |

发布门禁建议采用“**基线 + 回归**”模式：新版本不要求一步到位完美，但必须不劣化关键指标，并对新增风险有明确的缓解措施与时间表。

### 10.4.5 安全测试环境与数据治理

为了避免在红队过程中引入新的安全风险，建议：
- 使用隔离的测试环境与测试凭证，禁止直连生产关键资源
- 使用脱敏/合成数据构造测试集，必要时由数据所有者审批
- 对测试过程本身做审计留痕，确保可复盘与可追责

红队演练负责发现“真实世界会发生的问题”，自动化评估负责把这些问题变成“持续不会回归的问题”。两者结合，才能形成可持续的 LLM 安全改进闭环。
